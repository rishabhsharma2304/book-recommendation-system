# -*- coding: utf-8 -*-
"""book.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U4DnNeBA2mX5iz3Gru3d7rbZ59bYJTlv
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

book = pd.read_csv('/content/drive/MyDrive/Book_data/BX-Books.csv' , delimiter =';' , error_bad_lines=False , encoding='latin-1')

user = pd.read_csv('/content/drive/MyDrive/Book_data/BX-Users.csv' , error_bad_lines=False , delimiter= ';' , encoding='latin-1')

rating = pd.read_csv('/content/drive/MyDrive/Book_data/BX-Book-Ratings.csv' ,delimiter=';', error_bad_lines= False , encoding ='latin-1')

book.shape

user.shape

rating.shape

user.columns

book.columns

rating.columns

combine_book_data= pd.merge(rating , user , on='User-ID')

combine_book_data = pd.merge(combine_book_data , book , on= 'ISBN')

combine_book_data.head()

column= ['Image-URL-S' , 'Image-URL-M', 'Image-URL-L' ]
combine_book_data= combine_book_data.drop(column , axis=1)

combine_book_data.head()

round(combine_book_data.isnull().sum() / len(combine_book_data) * 100, 4)

sns.displot(combine_book_data['Age'] )

combine_book_data['Year-Of-Publication'] = pd.to_numeric(combine_book_data['Year-Of-Publication'] , errors='coerce').fillna(2099, downcast = 'infer')

combine_book_data['Book-Rating'] = combine_book_data['Book-Rating'].replace(0, None)

combine_book_data['Age'] = np.where(combine_book_data['Age']>90, None, combine_book_data['Age'])

combine_book_data[['Publisher' ,  'Book-Author']]= combine_book_data[['Publisher',  'Book-Author']].fillna('Unknown')

median = combine_book_data["Age"].median()
std = combine_book_data["Age"].std()
is_null = combine_book_data["Age"].isnull().sum()
rand_age = np.random.randint(median - std, median + std, size = is_null)
age_slice = combine_book_data["Age"].copy()
age_slice[pd.isnull(age_slice)] = rand_age
combine_book_data["Age"] = age_slice
combine_book_data["Age"] = combine_book_data["Age"].astype(int)

combine_book_data['Country'] = combine_book_data['Location'].apply(lambda row: str(row).split(',')[-1])

combine_book_data['Country'].head()

combine_book_data = combine_book_data.drop('Location', axis=1)

count_user = combine_book_data['User-ID'].value_counts()
combine_book_data= combine_book_data[combine_book_data['User-ID'].isin(count_user[count_user>=200].index)]
count_book_rat = combine_book_data['Book-Rating'].value_counts()
combine_book_data= combine_book_data[combine_book_data['Book-Rating'].isin(count_book_rat[count_book_rat>=100].index)]

combine_book_data.shape

combine_book_data = combine_book_data.dropna(axis=0 , subset=['Book-Title'])

rating_count= combine_book_data.groupby('Book-Title')['Book-Rating'].count()
rating_count.reset_index().rename(columns={'Book-Rating':'Rating-Count'}) [['Book-Title' , 'Rating-Count']]

combine_book_data.head()

c= ['Book-Author','Year-Of-Publication','Publisher','Age','Country']
combine_book_data= combine_book_data.drop(c , axis=1)

df=combine_book_data.merge(rating_count,on = 'Book-Title' ,how='inner')

df.head()

df.rename(columns={'Rating-Count':'Rating_Count'}, inplace=True)

final_data=df.query('Rating_Count >= @threshold')

final_data.head()

final_data.shape

from scipy.sparse import csr_matrix
final_data=final_data.drop_duplicates(['User-ID','Book-Title'])
df_p = final_data.pivot_table(index='Book-Title', columns='User-ID', values='Rating_Count').fillna(0)
fd_m= csr_matrix(df_p.values)

df_p.head()

from sklearn.neighbors import NearestNeighbors

model_knn=NearestNeighbors(metric='cosine' , algorithm='brute')
model_knn.fit(fd_m)

query_index= np.random.choice(df_p.shape[0])

print(query_index)

dist,ind = model_knn.kneighbors(df_p.iloc[query_index,:].values.reshape(1,-1) , n_neighbors=11)

df_p.index[query_index]

for i in range(0 ,len(dist.flatten())):
  if i==0:
    print('Recommendation for{0}: '.format(df_p.index[query_index]))
  else:
    print('{0}: {1}, with distances of {2}:'.format(i,df_p.index[ind.flatten()[i]], dist.flatten()[i]))



